% Template for a Computer Science Tripos Part II project dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[nameinlink]{cleveref}
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage{todonotes}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{tabularx}

%%%%%%% Using Minted Package for Code Listings %%%%%%%
\usepackage[outputdir=.texpadtmp]{minted}
\usemintedstyle{colorful}

\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable
                                        
\newcommand{\mytodo}{\todo[inline, color=green!40]}

\begin{document}

\bibliographystyle{plain}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title


\pagestyle{empty}

\rightline{\LARGE \textbf{Rupert Horlick}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{Encrypted Keyword Search Using Path ORAM on MirageOS} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Homerton College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Rupert Horlick                       \\
College:            & \bf Homerton College                     \\
Project Title:      & \bf Encrypted Keyword Search Using \\
& \bf Path ORAM on MirageOS \\
Examination:        & \bf Computer Science Tripos -- Part II, July 2016  \\
Word Count:         & \bf \footnotemark[1]
                      (well less than the 12000 limit)  \\
Project Originator: & Dr Nik Sultana                    \\
Supervisors:         & Dr Nik Sultana \& Dr Richard Mortier                    \\ 
\end{tabular}
}
\footnotetext[1]{This word count was computed
by \texttt{detex diss.tex | tr -cd '0-9A-Za-z $\tt\backslash$n' | wc -w}
}
\stepcounter{footnote}


\section*{Original Aims of the Project}

% Give a 100 word summary of what was to be achieved by the project, i.e. secure searchable encrypted documents

\section*{Work Completed}



\section*{Special Difficulties}


 
\newpage
\section*{Declaration}

I, Rupert Horlick of Homerton College, being a candidate for Part II of the Computer
Science Tripos, hereby declare
that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation
does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Signed [signature]}

\medskip
\leftline{Date [date]}

\tableofcontents

\listoffigures

\todototoc
\listoftodos

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\pagestyle{headings}

\chapter{Introduction}

% Give an overview of the problem - similar to the beginning of the project proposal

% Talk about increasing use of cloud computing

% Talk about loss of ability to search efficiently over documents

% Define threat model clearly

% Talk about solutions based on symmetric encryption

% Explain why these fail and what the consequences are

% Discuss ORAM as a solution to this problem

% Describe other work in the area of ORAM etc.

\section{Motivation}

Cloud computing is becoming ubiquitous, with more than an exabyte of data estimated to be stored in the cloud. For large businesses, a private cloud can be a cost effective way to keep data isolated, but as public cloud services become ever cheaper, even these businesses could be forced to succumb to market pressures and move to the public cloud. With so much important data held by only a few major cloud providers, trust becomes a major issue.

Encryption appears to be the solution to our trust issues; if the providers cannot read the plain-text of our data then surely it is secure? This appears to hold in general, but in the important application of query-based searching, we have a problem: using current methods of homomorphic encryption to perform search over encrypted documents can leak up to 80\% of queries \cite{islam2012access}. Knowledge about the queries made to a data set, along with the amount of documents returned by each query, could lead to some dangerous inferences. As a motivating example, discovering that a query such as $\langle name,~disease\rangle$ made to a medical database returned results would allow an adversary to uncover information about a patients medical status, breaching patient confidentiality.

What allowed the authors of \cite{islam2012access} to infer search queries was knowledge about the documents returned by any specific query. Thus, in order to protect against this kind of attack we need to have some way of preventing the server from knowing which documents it is actually returning in response to any query. Oblivious Random-Access Memory (ORAM) gives us exactly what we want. When using ORAM, not only are two accesses made to exactly the same piece of data computationally indistinguishable to the server, but so too are any two access patterns of the same length.

This project aims to demonstrate that, using a particular ORAM protocol Path ORAM \cite{stefanov2013path}, it is possible to build a system that allows us to search over a set of encrypted documents without leaking the resulting access pattern, protecting the content of the search queries and therefore the confidentiality of the documents.

\section{Challenges}

The first major challenge that faces any security related project is adequately defining the threat model. In order to be able to reason about and evaluate the security properties of the system, we need to know exactly what we assume an adversary to be capable of. Once we have done this we must show that within these capabilities, the security properties that we desire the system to have remain intact. The threat model will be defined in \cref{sec:threatmodel}.

By virtue of being stored in the cloud, we should be able to access our data at any time, from any place, while still maintaining the desired security properties. We also want to be tolerant of network connection errors and client-side crashes. Thus, another challenge is to make the system completely stateless.

In order to make statelessness more efficient, it is necessary to augment ORAM with recursion (\cref{sec:oramintro}), which reduces the amount of transient client-side storage. This enables us to efficiently store the client's state between accesses. Recursion introduces the challenge of choosing how many levels to use. Each extra level of recursion reduces the size of the client's state, but also incurs a time and space overhead. This is explored briefly in \cite{stefanov2013path}, but we will attempt to have the system automatically choose parameters for the recursion based on the size and block size of the underlying storage used by the system.

Finally in order for the system to perform query-based search over encrypted documents we need to implement a file system, an information retrieval module and an encryption module. These modules will be rudimentary, as the main focus of the project is the implementation and optimisation of the ORAM module, but all of them have inherent design and implementation challenges that will be need to be addressed.

\section{Related Work}

\mytodo{Explore and discuss related work}

\chapter{Preparation}

% Describe all work undertaken before the coding - show professional approach to project

% Talk about refining and clarifying model

% Talk about reviewing research papers to find best way of bringing things together

% Talk about choosing OCaml 	for its static typing and powerful module system

% Talk about how this led to designing the system in a modular way (include figure for system)

% Talk about MirageOS and building on top of it

\section{Defining the Threat Model}
\label{sec:threatmodel}

The threat model is defined in terms of a client, a server and an attacker.

\setlength{\unitlength}{0.67mm}
\input{threatModel}
\setlength{\unitlength}{0.5mm}

We assume that the network is under the attacker's control. In the basic model we define the server and the attacker to be passive, and honest, but curious. This means that they will both gather as much information as possible, without deviating from the protocol. Thus the attacker will eavesdrop, but will not prevent messages from arriving at the server, tamper with them in any way or produce their own fake messages. The server will also not tamper with the messages, or with the underlying storage that the protocol is accessing. In this model the attacker is not as interesting as the server, because all communications are encrypted. Thus it is the server, that can see the access patterns of the underlying storage, that we are trying to protect against. The goal of the project is to make sure that this access pattern reveals nothing about the contents of the search queries or the documents in the underlying storage.

We can also extend the model to one where both the attacker and the server become active, freely tampering with messages and introducing new messages, and the server tampers with the underlying storage. In this case we will need to extend our solution with integrity verification.

\section{Introduction to Path ORAM}
\label{sec:oramintro}

Path ORAM is defined in terms of a client and a server, where the client wishes to store data on the server. The data is split into blocks and each block is tagged with a sequence number or address, its position if the data were to be stored sequentially. On the server the data is stored in a binary tree of height $L$, where each node in the tree is a bucket with size $Z$, containing up to $Z$ data blocks. The client requires two local data structures. The stash is a sort of working memory; as data is read from the server it is put into the stash and it is then written back to the server later on. The position map associates with each address a number between $0$ and $2^L-1$, which corresponds to a leaf in the tree. The protocol maintains the invariant that, at anytime, a block with position $x$ in the position map is either in the stash, or in a bucket along the path from the root of the tree to the $x^{th}$ leaf.

This is achieved using \cref{alg:access}, which is split into four main steps:

\begin{description}
	\item[Remap Block] The current position of the block $\mathsf{a}$ is read and then a new position is calculated uniformly at random
	\item[Read Path] The path to the leaf $x$ is read into the stash. At this point the block for address $\mathsf{a}$ is definitely in the stash, if it has ever been written into the ORAM
	\item[Write New Data] If the operation is a $\mathsf{write}$, then the value in the stash is replaced by the new $\mathsf{data^*}$
	\item[Write Path] The path to the leaf $x$ is filled with blocks from the stash. A block with address $\mathsf{a'}$ can be put in the bucket at level $l$, if the path to $\mathsf{position[a']}$ intercepts the path to $x$ at level $l$. If $min(|S'|,Z) < Z$, then the bucket is filled with dummy blocks
\end{description}

Clearly after this, either it was possible to write a block into the stash along the path to its leaf, or not, in which case it is in the stash, and the invariant holds.

\begin{algorithm}[h]
\caption{Read/write data block at address $\mathsf{a}$}
\label{alg:access}
\begin{algorithmic}[1]
    \vskip 10pt
    \Function{Access}{$\mathsf{op,a,data^*}$}
    \vskip 10pt
    \State $x \gets \mathsf{position[a]}$
    \State $\mathsf{position[a]} \gets$ \Call{UniformRandom}{$2^L-1$}
    \vskip 10pt
    \For{$l \in \{0,1,\dots,L\}$}
    	\State $S \gets S~\cup$ \Call{ReadBucket}{$\mathcal{P}(x,l)$}
    \EndFor
    \vskip 10pt
    \State $\mathsf{data} \gets$ Read block $\mathsf{a}$ from $S$
    \If{$\mathsf{op} = \mathsf{write}$}
    	\State $S \gets (S - \{(\mathsf{a,data})\}) \cup \{(\mathsf{a,data^*})\}$
    \EndIf
    \vskip 10pt
    \For{$l \in \{L,L-1,\dots,0\}$}
    	\State $S' \gets \{(\mathsf{a',data'}) \in S : \mathcal{P}(x,l) = \mathcal{P}(\mathsf{position[a']},l)\}$
    	\State $S' \gets$ Select $\min(|S'|,Z)$ blocks from $S'$
    	\State $S \gets S - S'$
    	\State \Call{WriteBucket}{$\mathcal{P}(x,l),S'$}
    \EndFor
    \vskip 10pt
    \State \Return $\mathsf{data}$
    \vskip 10pt
    \EndFunction
    \vskip 10pt
\end{algorithmic}
\end{algorithm}

The security of this operation comes from the fact that positions are assigned uniformly at random. If we consider the sequence of accesses, $$\mathbf{p} = (\mathsf{position}_M[\mathsf{a}_M], \mathsf{position}_{M-1}[\mathsf{a}_{M-1}], \dots, \mathsf{position}_1[\mathsf{a}_1]),$$ any two accesses to the same address will be statistically independent and trivially so will two accesses to different addresses. Thus, by an application of Bayes' rule, $$\Pr(\mathbf{p}) = \prod\limits^{M}_{j=1}\Pr(\mathsf{position}_j[\mathsf{a}_j]) = \left(\frac{1}{2^L}\right)^M$$ and the access pattern is indistinguishable from a random sequence of bit strings.

We might, and for our application will, want to make ORAM stateless, storing all information on the disk, and nothing (except private keys) on the client. We can do this na\"ively by simply dumping all of the state to disk after every operation, but in our current construction, the position map takes $O(n)$ storage space on the client, and the stash $O(\log N)$. Adding recursion to ORAM allows us to reduce the size of the overall client storage to $O(\log N)$, by reducing the space required for the position map to $O(1)$, while maintaining $O(\log N)$ for the stash (actually now multiple stashes). We can now write the client-side storage to the server after each call, and read it back again before the next one, without increasing the asymptotic bandwidth overheads.

Recursion works by storing the position map of our original ORAM, now referred to as ORAM$_0$, in a new ORAM, ORAM$_1$. Now the client stores the position map of this ORAM, plus the stashes of both. If we can store $\chi$ positions in each block of ORAM$_1$, then the new position map is only $O(N/\chi)$. We can then repeat this process until we have the position map of ORAM$_n$ being of constant size. The number of levels required to achieve this are examined in \cite{stefanov2013path} and explored further in \cref{subsec:comparisonWithLiterature}.

\section{Introduction to Inverted Indexes}
\label{sec:invertedindexintro}

The inverted index is the single most important data structure in Information Retrieval. It allows us to perform a large amount of work in advance in order to give performance that is linear in the number of documents involved in a query, much better than a linear scan of all documents that is $O(NK)$, for $N$ documents with an average of $K$ terms. It consists of two parts: the dictionary and the postings. The dictionary is a list, usually stored as a hash table, of all of the terms that appear in a set of documents. Then for each term, we have a postings list, a list of all the documents that contain a term. Collectively these postings lists are referred to as the postings.

So now looking up a single keyword is as simple as hashing the keyword and returning the relevant postings list, if it exists. Simple boolean operations are also easy to support. Disjunction simply takes the union of two postings lists and conjunction the intersection.

In this project we will restrict operations to simple, space-separated disjunctions, because we are focusing on showing the correctness and efficiency of search using the ORAM implementation, rather than creating an advanced IR system.

Constructing an Inverted Index can be done in three simple steps. We first tokenise the text that we want to add to the index. We then perform some linguistic preprocessing on these tokens, for example stemming, to get them into some sort of normalised form. Finally, we add the document ID of the document to the postings list of each token.

\section{Introduction to MirageOS}

MirageOS is a unikernel operating system designed to run on the Xen hypervisor. The idea is to provide the minimum amount of functionality required of an application by pulling together a number of libraries, and compiling down to an executable that runs directly on the hypervisor. Compared to the traditional approach of running a full operating system such as Ubuntu, with databases, web servers, and the like built on top, the unikernel provides significant speed and memory improvements.

Building the implementation of ORAM on Mirage allows us to have a consistent interface between client machines and cloud providers. We can run the ORAM software on a trusted cloud instance and connect through it from any client machine, to any cloud provider that we like. We also don't need to have the instance running at all times. Mirage is so lightweight, that we could spin up an instance every time we need to perform some actions. This saves us money because, using cloud services like AWS, we only pay for the compute time we actually use. This is the main motivation behind implementing statelessness in the project.

\section{System Architecture}

\setlength{\unitlength}{0.75mm}
\input{mirageStack}
\setlength{\unitlength}{0.5mm}

\mytodo{Give an overview of the system level design including the figure from the proposal}

% Stuff to do with higher level architectures and clients and cloud server

\section{Requirements Analysis}

% Do requirements analysis in order to show that I thought about all of these things

\begin{description}
	\item [High Priority] Basic ORAM implementation
	\item [Medium Priority] File System
	\item [Medium Priority] Search Module
	\item [Low Priority] Encryption
	\item [Low Priority] Further Optimisations including Recursion and Statelessness
	\item [Low Priority] Integrity Verification (Extension)
\end{description}

\mytodo{Further discuss the requirements identified above}

\section{Choice of Tools}

\subsection{OCaml}

Having decided to build ORAM on top of MirageOS, OCaml was really the only choice of programming language, because Mirage applications and libraries are all built in OCaml. However, OCaml was also part of the reason for choosing to build on Mirage. 

OCaml has an extremely powerful module system, making it easy to parameterise the ORAM implementation over module signatures for Block devices or other system components. This not only encourages more generalised programming, but also allows powerful techniques like recursive modules to be exploited. For instance, to implement recursive ORAM, the position map of the main ORAM is another ORAM, so we can simply parameterise over a position map signature and pass the ORAM implementation to itself as the position map.

OCaml's static typing system is also indispensable for ensuring correctness of programs and increasing productivity.

\subsection{Libraries}
\label{subsec:libraries}

\mytodo{Add a small amount of discussion to this section}

\begin{table}[h]
\centering
\begin{tabularx}{\textwidth}{|l|l|X|l|}
\hline
\textit{Library} & \textit{Version} & \textit{Purpose} & \textit{License} \\
\hline \hline
Mirage & 2.6.1 & System Component Interface Definitions, Application Configuration Framework & ISC \\
\hline
Jane Street's Core & 112.35.00 & Data Structures, Algorithms & Apache-2.0 \\
\hline
LWT & 2.5.1 & Threading & LGPL-2.1 \\
\hline
Cstruct & 1.7.1 & Data Structure & ISC \\
\hline
Alcotest & 0.4.6 & Unit Testing & ISC \\
\hline
\end{tabularx}
\caption{Libraries used by Mirage ORAM}
\label{tab:libraries}
\end{table}

\subsection{Development Environment}

\mytodo{Add a small amount of discussion to this section}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textit{Tool} & \textit{Version} & \textit{Purpose} & \textit{License} \\
\hline \hline
Mac OSX & 10.11.2 & Operating System & Proprietary \\
\hline
Atom & 1.3.2 & Text Editor & MIT \\
\hline
OPAM & 1.2.2 & Package Manager & GPLv3 \\
\hline
OASIS & 0.4.5 & Build Tool & LGPL-2.1 \\
\hline
\end{tabular}
\caption{Tools used in the development of Mirage ORAM}
\label{tab:devtools}
\end{table}

% Do a table of tools used including OPAM, OASIS, git, Make, Atom, etc.

\section{Software Engineering Techniques}

% Talk about writing interface files before writing code in order to fit code to requirements and designing before implementing

% Talk about the ability to do Test Driven Development and the power of unit tests

I employed two major techniques to ensure my code was well thought through and well built, while remaining productive. 

First of all a Test Driven Development approach allowed me to fail fast. I wrote unit tests for each new piece of code that was written, until I was sure that it was working correctly. This meant that when it came to combining small modules into a larger system, things worked together as expected more often than not.

Secondly, I wrote interface files before writing the actual implementation. This meant that I had to think about the design of each module thoroughly before writing any code and also meant that I could make adjustments to other modules in order to fit with the new design.

Combining these techniques with documentation and structuring of both the source code and the source repository, led to a manageable and feasible development workflow.

\section{Summary}

In this chapter I have discussed the work that was undertaken before development began. This included a rigorous definition of the threat model, a brief introduction to the major algorithms, data structures and libraries, an overview of preliminary architectural designs, and a discussion of the techniques and tools used in the development process.

The next chapter will show how all of this information was used to achieve the aims of the project.

\chapter{Implementation}

This chapter describes the process that took the designs and algorithms of the previous chapter and turned them into a functioning system. As it is the core of the project, the implementation of Path ORAM is discussed first (\cref{sec:pathORAM}), followed by the file system (\cref{sec:fileSystem}), the search module (\cref{sec:searchmodule}) and finally encryption (\cref{sec:encryption}).

% Describe what was actually implemented

\section{Path ORAM}
\label{sec:pathORAM}

The structure of Path ORAM is described abstractly in \cref{sec:oramintro}, in terms of the core data structures and the access algorithm. In this section we discuss how those data structures were realised and the design decisions involved in the implementation.

\subsection{Inherent Constraints}
\label{subsec:constraints}

By virtue of writing an implementation to satisfy an existing module signature, there are a number of constraints put on the design of our system.

The first major constraint is the use of the Cstruct library, discussed in \cref{subsec:libraries}. The underlying block device requires a buffer of type \texttt{Cstruct.t} to read to/write from and in order to satisfy the \texttt{BLOCK} module signature, ORAM needs to return data as a \texttt{Cstruct.t}. Thus, to avoid unnecessary conversions, we will pass our data around in this form.

Another constraint is the usage of \texttt{int64} as the type of addresses in \texttt{BLOCK}'s, \texttt{read} and \texttt{write} operations. Again, to avoid unnecessary (and potentially unsafe) work converting between types, we will use \texttt{int64}s wherever necessary.

\subsection{Stash}

The stash stores blocks of data temporarily before they are written back into ORAM. It needs to support operations of insertion, lookup based on address and removal. For this job I chose an \texttt{int64}-keyed hash table from Jane Street's Core library, with \texttt{Cstruct.t} values. This was put into its own module, abstracting away the underlying type, meaning that we could swap implementations of the Stash module without breaking the core code of the ORAM module.

The hash table gives us constant time for the operations of insertion, lookup and removal, making it ideal for our needs. The hash table implementation takes an initial size as a parameter, and expands when necessary. This would add a large overhead, because we would have to copy the entire contents of the stash. However, as shown in \cite{stefanov2013path}, we require exactly $Z\log_2N$ transient storage, where $N$ is the size of the ORAM in blocks, and on top of that, we require a constant amount of space for persistent stash storage. \Cref{tab:stashsizes} shows the maximum stash size required depending on the security parameter, $\lambda$, and the bucket size $Z$. A stash with security parameter $\lambda$ has probability $2^{-\lambda}$ of exceeding this stash size.

In order to achieve statelessness, we need to be able to write the stash to disk, so there is going to be a trade-off between maximum stash size and security parameter. A larger maximum stash size increases bandwidth, but a security parameter set too low will not allow us to allocate the storage space for the stash in advance. For the purposes of this project, we will parameterise in both bucket size and security parameter, allowing us to discover empirically the values that optimise the construction.

\begin{table}
\centering
\begin{tabular}{|l|l|l|l|}
	\hline
	& \multicolumn{3}{c|}{Bucket Size ($Z$)} \\
	\cline{2-4}
	Security Parameter ($\lambda$) & 4 & 5 & 6 \\
	\cline{2-4}
	& \multicolumn{3}{c|}{Max Stash Size} \\
	\hline
	80 & 89 & 63 & 53 \\
	\hline
	128 & 147 & 105 & 89 \\
	\hline
	256 & 303 & 218 & 186 \\
	\hline
\end{tabular}
\caption{Empirical results for maximum persistent stash size}
\label{tab:stashsizes}
\end{table}

\subsection{Position Map}

The position map associates a leaf position with each address of the data. As mentioned in \cref{subsec:constraints}, we are constrained to using \texttt{int64} addresses, thus we need to be able to support a position map that is indexed by 64-bit integers. OCaml provides us with a Bigarray module, but the size of these arrays is specified using the OCaml \texttt{int} type. This type actually uses only 63 bits on a 64-bit machine and 31 on a 32-bit machine. Both of these types are also signed, so we need to represent a type that can range up to $2^{63} - 1$, using a type that can only go up to $2^{30} - 1$ on a 32-bit machine.

In order to do this we need to use 3-dimensional arrays. We take an \texttt{int64} value and split its bits into a 4-bit value and 2 30-bit values. The 4-bit value is the most significant bits, and will therefore be 0 unless we store more than $2^{60}$ blocks. The 30-bit values are guaranteed to be converted into positive \texttt{int}s, which we can then use to address two dimensions of the array. We have to perform some input sanitation to make sure that the 4-bit value (and therefore the 64-bit value) is positive.

Now the only remaining problem is creating the position map. \Cref{alg:posmapdims} shows how we translate from a desired \texttt{int64} size to the dimensions of a three dimensional array. After splitting the \texttt{int64} as described above, we must add one to the first two dimensions, because we always want them to be at least of size 1. If a higher dimension is greater than 1, then all lower dimensions become their maximum value, in this case $2^{30}-1$. We can now create an array using these values that is guaranteed to be at least the size that we require on both 32-bit and 64-bit machines.

\begin{algorithm}
\caption{Calculate the dimensions of a 3D array given total desired size}
\label{alg:posmapdims}
\begin{algorithmic}[1]
\vskip 10pt
\Require{$\mathsf{size} > 0$}
\vskip 10pt
\Function{PosMapDims}{$\mathsf{size}$}
\vskip 10pt
	\State $(x, y, z) \gets$ \Call{SplitIndices}{$\mathsf{size}$}
\vskip 10pt
	\State $x \gets x + 1$
	\State $y \gets y + 1$
\vskip 10pt
	\If{$x > 1$}
		\State $y \gets \mathsf{0x3FFFFFFF}$
		\State $z \gets \mathsf{0x3FFFFFFF}$
	\ElsIf{$y > 1$}
		\State $z \gets \mathsf{0x3FFFFFFF}$
	\EndIf
\vskip 10pt
	\State \Return $(x,y,z)$
\vskip 10pt
\EndFunction
\vskip 10pt
\end{algorithmic}
\end{algorithm}

\subsection{Creating ORAM}

We want ORAM to be able to replace any existing block device in any Mirage program. In order to do this we need access to the methods of the underlying block device as well as the block device itself. Thus, we need to build an ORAM functor, that takes a module satisfying the \texttt{BLOCK} interface, shown in \cref{lst:blocksig}, and gives us a new module satisfying the same interface.

In order to get access to this underlying device, we need a \texttt{create} method, which takes a block device as input and returns something of type \texttt{Oram.Make(B).t}, shown in \cref{lst:orammaketype}. This type contains the ORAM parameters such as \texttt{bucketSize} and \texttt{offset}, structural information such as the \texttt{height} of the ORAM and the \texttt{numLeaves}, and pointers to the stash, position map, and underlying block device. 

The following parameters are passed as input to the \texttt{create} method, along with the block device:

\begin{description}
  \item[\texttt{size}] The desired size of the ORAM in blocks
  \item[\texttt{blockSize}] The desired size of a single block in bytes
  \item[\texttt{bucketSize}] The number of blocks in a bucket
\end{description}

Using these, the \texttt{create} method can calculate new structural information. First we need to calculate the number of sectors required for a block as $$\mathtt{sectorsPerBlock} = \frac{\mathtt{blockSize} - 1}{\mathtt{sector\_size}} + 1,$$ which rounds up the number of sectors so we can always fit the desired block size. Next, we calculate the sector size of the ORAM as $$\mathtt{sector\_size} = \mathtt{sector\_size} \times \mathtt{sectorsPerBlock} - 8,$$ which is the block size, minus 8 bytes for the address, which needs to be stored along with the data. Now we can calculate the height of the ORAM, but we need to consider two cases. If the desired size of the ORAM is specified, then we simply calculate the height as $$\mathtt{height} = \left\lfloor \log_2\left(\frac{\mathtt{size}}{\mathtt{bucketSize}} + 1\right)\right\rfloor - 1.$$ This comes from rearranging the equation for the size of the binary tree, $2^{\mathtt{height} + 1} - 1$, introducing the floor operator so that the resulting binary tree is less than or equal to the desired size. If the size is unspecified, we assume that we should fill as much of the device as possible, so we calculate $$\mathtt{size} = \frac{\mathtt{size\_sectors}}{\mathtt{sectorsPerBlock}}$$ and then perform the same calculation as above with this new value. Finally we calculate, \texttt{numLeaves} and a new value for \texttt{size\_sectors} trivially.

This is all of the structural information we require, so now we only have a couple of things left to do. We must create instances of the client-side data structures and we must initialise the ORAM space. The first simply calls the creation functions of the associated data structures. The second loops through the block device, writing dummy blocks to every location. Dummy blocks have address -1, and are ignored by the access protocol. Now we can return everything that we have as an instance of \texttt{ORAM.Make(B).t}.

\begin{listing}[H]
\caption{The type of an ORAM device \texttt{ORAM.Make(B).t}}
\label{lst:orammaketype}
\inputminted[firstline=27, lastline=39]{ocaml}{../src/oram.ml}
\end{listing}


\begin{listing}
\caption{Mirage BLOCK module signature}
\label{lst:blocksig}
\vskip 10pt
\begin{minted}[breaklines]{ocaml}
module type BLOCK = sig

  type page_aligned_buffer = Cstruct.t

  type +'a io = 'a Lwt.t

  type t

  type error = [
    | `Unknown of string (** an undiagnosed error *)
    | `Unimplemented     (** operation not yet implemented in the code *)
    | `Is_read_only      (** you cannot write to a read/only instance *)
    | `Disconnected      (** the device has been previously disconnected *)
  ]

  type id

  val disconnect: t -> unit io

  type info = {
    read_write: bool;    (** True if we can write, false if read/only *)
    sector_size: int;    (** Octets per sector *)
    size_sectors: int64; (** Total sectors per device *)
  }

  val get_info: t -> info io

  val read: t -> int64 -> page_aligned_buffer list -> [ `Error of error | `Ok of unit ] io

  val write: t -> int64 -> page_aligned_buffer list -> [ `Error of error | `Ok of unit ] io

end
\end{minted}
\end{listing}

\subsection{Accessing ORAM}

Now that we have the client-side data structures and our ORAM module, we can almost implement \cref{alg:access}. Before we can though, we need the surrounding plumbing. The \texttt{BLOCK} interface functions \texttt{read} and \texttt{write} input/output data as a list of \texttt{Cstruct.t}s, so they must split this into chunks before calling \texttt{access} on each one. On the other side of access, we need the subroutines \textsc{ReadBucket} and \textsc{WriteBucket}, which access the underlying block storage.

It is the second two of these plumbing functions that have a much more interesting role. They are actually responsible for maintaining the logical binary tree structure. There are no physical pointers, but instead the structure is built purely through calculating the appropriate address of a bucket. The bucket on the path to leaf $x$ at level $l$ is calculated using \cref{alg:bucketaddress}.

\begin{algorithm}[H]
  \begin{algorithmic}
  \vskip 10pt
    \Function{BucketAddress}{$x$,$l$}
      \State $address \gets 0$
      \For{$i = 0;~i < l;~i++$}
        \If{$x >> (i + \mathtt{height} - l)~\&\&~1 = 1$}
          \State $address \gets (2 \times address) + (\mathtt{bucketSize} \times \mathtt{sectorsPerBlock} \times 2)$
        \Else
          \State $address \gets (2 \times address) + (\mathtt{bucketSize} \times \mathtt{sectorsPerBlock})$
        \EndIf
      \EndFor
      \State \Return $address$
    \EndFunction
  \vskip 10pt
  \end{algorithmic}
  \caption{Calculating the address of the bucket at level $l$ on the path to leaf $x$}
  \label{alg:bucketaddress}
\end{algorithm}

This is most easily explained using \cref{fig:bucketaddress}. Here we can see the nodes labelled in order of their position in memory. The leaves are also labelled, but the binary representations of these labels are actually more important. The binary representation, read from left to right, tells us how to reach the leaf. To get to leaf 5, with binary representation 101, we go right, left, right. When we go left, we double the label on the node and add one, and when we go right, we add two. Multiplying this node label by the block size and the bucket size gives us the physical address of the node. So for a node at level $l$, we only compare the first $l$ bits, following the procedure above.

\begin{figure}[h]
  \centering
    \begin{picture}(310,160)
    \put(140,140){\framebox(30,20){0}}
    \put(155,140){\vector(4,-1){78}}
    \put(155,140){\vector(-4,-1){78}}
    \put(197,135){1}
    \put(60,100){\framebox(30,20){1}}
    \put(75,100){\vector(2,-1){38}}
    \put(75,100){\vector(-2,-1){38}}
    \put(220,100){\framebox(30,20){2}}
    \put(235,100){\vector(2,-1){38}}
    \put(198,95){0}
    \put(235,100){\vector(-2,-1){38}}
    \put(20,60){\framebox(30,20){3}}
    \put(35,60){\vector(1,-1){19}}
    \put(35,60){\vector(-1,-1){19}}
    \put(100,60){\framebox(30,20){4}}
    \put(115,60){\vector(1,-1){19}}
    \put(115,60){\vector(-1,-1){19}}
    \put(180,60){\framebox(30,20){5}}
    \put(195,60){\vector(1,-1){19}}
    \put(195,60){\vector(-1,-1){19}}
    \put(215,50){1}
    \put(260,60){\framebox(30,20){6}}
    \put(275,60){\vector(1,-1){19}}
    \put(275,60){\vector(-1,-1){19}}
    \put(0,20){\framebox(30,20){7}}
    \put(40,20){\framebox(30,20){8}}
    \put(80,20){\framebox(30,20){9}}
    \put(120,20){\framebox(30,20){10}}
    \put(160,20){\framebox(30,20){11}}
    \put(200,20){\framebox(30,20){12}}
    \put(240,20){\framebox(30,20){13}}
    \put(280,20){\framebox(30,20){14}}
    \put(12,0){0}
    \put(52,0){1}
    \put(92,0){2}
    \put(132,0){3}
    \put(172,0){4}
    \put(212,0){5}
    \put(252,0){6}
    \put(292,0){7}
  \end{picture}
  \caption{Visualisation of \cref{alg:bucketaddress}}
  \label{fig:bucketaddress}
\end{figure}

So we can now input/output data from the outside and the underlying block device. This leaves most of \cref{alg:access} fairly trivial to implement. Remapping the blocks requires a simple call to a pseudo-random function, reading in the path just adds the contents of each bucket to the stash, and writing new data is as simple as setting a value in the stash. The only interesting part left is lines 11-16. This is where we decide which blocks should be put into the path.

The naive implementation of this, and that suggested by \cref{alg:access}, loops through the stash and finds blocks such that the bucket at level $l$ on the path to leaf $\mathsf{position[a']}$ is the same as the bucket at level $l$ on the path to leaf $x$. There are two small optimisations we can make here. The first is to perform the position lookup only once, attaching positions to the blocks in a temporary data structure, avoiding repeated work at each level. The second avoids calculating the bucket addresses entirely. We can do this by noting that in order for the paths to two leaves to intersect at level $l$, the leaves must have the same first $l$ bits. Thus, we can simply perform a right bit shift on both $x$ and $\mathsf{position[a']}$ of $\mathtt{height} - l$ bits, and check for equality.

So at this point we have a functioning ORAM functor, that can be used to augment a block device in any Mirage program. In \cref{subsec:recursion,subsec:statelessness,subsec:oramoptimisation} we look at optimisations and extensions to this ORAM construction, before moving onto to the further aims of the project.

%\begin{listing}[h]
%	\caption{Implementation of \cref{alg:access}}
%	\label{lst:access}
%	\inputminted[firstline=183, lastline=207]{ocaml}{../src/oram.ml}
%\end{listing}

\subsection{Recursion}
\label{subsec:recursion}

The essence of recursive ORAM, is that the position map of one ORAM is another ORAM. Thus, we can extend our original ORAM functor, parameterising it in the implementation of the position map. To do this we use a single interface, that is satisfied by both the in memory position map and the ORAM module. We can apply this new ORAM functor with the in memory position map module to get our original construction. However, we can now take this further, applying the functor again with the result of the first application, and this can be done to an arbitrary depth. Recursion has been made easy using the power of OCaml's module system.

There is of course hidden complexity here. How do we create a recursive ORAM? We definitely can't assume that the data ORAM can use the whole of the disk anymore, so we will have to do some calculations. How do we initialise recursive ORAM? How do we actually use ORAM as a position map? We will examine all of these now.

\mytodo{In particular talk about/show maths involved in automatically determining optimal size/number of levels of recursion}

\subsection{Statelessness}
\label{subsec:statelessness}

\mytodo{Explain the implementation of Statelessness}

\subsection{Integrity Verification}

\mytodo{If had time to do integrity verification then include here otherwise discuss why I chose not to include it}

\section{File System}
\label{sec:fileSystem}

In order to perform search over documents, we need some way of actually storing those documents. This section describes the design and implementation of a basic file system that satisfies the requirements of the project.

\subsection{General Design}

The most common way of building a file system on top of a block device is through the use of inodes. An inode contains meta-information about a file along with pointers to the actual data blocks. For the purposes of this project, an inode will simply be one sector of the block device, containing the length of the file, followed by the list of pointers. In a system with more complex needs the inode would contain more information, such as modification/access timestamps, file permissions, etc., but we simply want to be able to read and write documents.

We need to be able to access the inode for a particular file quickly, so we should store its location in an index. We could perform lookup based on the actual filename, but the names have variable lengths, therefore, because we want to store the index, it is better to lookup based on the hash of the filename. \Cref{subsec:inodeindex} describes the implementation of the Inode Index.

We also need to allocate space on the block device for inode index blocks, for inode blocks and for data blocks. So we need a map that tells us which blocks on the device are free and allows us to update it as new blocks are needed. \Cref{subsec:freemap} describes the implementation of the Free Map.

We could now build a working file system, but we want it to be stateless. In order to do that, we need to store enough information to be able to reconstruct its in-memory representation. All we actually need to store is the root address of the Inode Index and the length of the Free Map. These two alone allow us to locate the data structures on disk when reconnecting to the block device. We store these two pieces of information at address 0 in what we call the Superblock.

\subsection{The Inode Index}
\label{subsec:inodeindex}

We need a data structure that associates keys, in the form of file hashes, with values, in the form of pointers to inodes. We want to support operations of insertion, lookup and deletion efficiently, but we also want to store the data structure on disk. This leads us naturally to an implementation using B-Trees.\footnote{The algorithms for B-Tree operations were adapted from \cite{CLRS09}}

B-Trees are a generalisation of self-balancing binary search trees, where each node can have more than one child. If a node has $n$ children, then it stores $n-1$ keys. It is guaranteed that $$ \forall k \in child_m, j \in child_{m+1} . k < key_m < j, $$ that is, a key is greater than all the keys to its left and less than all the keys to its right. 

B-Trees are an efficient on-disk data structure, because we can use the whole of a disk sector for one node. This gives us an extremely high branching factor, reducing the depth of the tree and therefore the number of disk sectors that we need to access in any single operation. On creation of the file system, we calculate the branching factor of the tree in order to fill as much of each sector as possible with useful information.

\subsection{The Free Map}
\label{subsec:freemap}

In order to allocate space efficiently, we can simply use an array of bits the size of the block device. We again want to have an on-disk data structure, or at least one that can easily be flushed to disk regularly. It was therefore beneficial to write my own bit array based on \texttt{Cstruct}s, rather than using a library implementation. This gives us the ability to write the whole structure directly onto the disk using the block device methods, without any cumbersome translation.

The \texttt{Cstruct} library performs data access in bytes. This leads us to \cref{alg:bitgetset} for getting and setting individual bits. To get the $n^{th}$ bit, we must get the $\frac{n}{8}^{th}$ byte and extract it from there. To do this, we calculate the index of the bit in the byte, shift a 1 to that position, and perform an and, masking that bit. Setting is a similar operation, but seeks to preserve the surrounding bits. To set a 1, we calculate the index of the bit in the byte, shift a 1 to that position, and perform an or, preserving all other bits. Setting a 0 is slightly trickier. We want to perform an and with a bit string that is 0 at the desired position and 1 everywhere else, but shifting fills empty bits with 0s. We can however use De Morgan's Law $$ a~\&\&~b = \neg (\neg a~||~\neg b) $$ to convert this to an operation involving a bit string that has a 1 at the desired position and 0s everywhere else.

\begin{algorithm}[H]
\caption{Getting and setting individual bits in a byte array}
\label{alg:bitgetset}
\begin{algorithmic}
\vskip 10pt
\Function{GetBit}{$\mathsf{index}$}
  \State $byte \gets \mathsf{byteArray[index]}$
  \State $shift \gets 7 - \mathsf{index} \bmod 8$
  \State \Return $byte >> shift~\&\&~1$
\EndFunction
\vskip 10pt
\Function{SetBit}{$\mathsf{index,boolean}$}
  \State $byte \gets \mathsf{byteArray[index]}$
  \State $shift \gets 7 - \mathsf{index} \bmod 8$
  \If{$\mathsf{boolean}$}
    \State $byte \gets byte~||~1 << shift$
  \Else
    \State $byte \gets \neg (\neg byte~||~1 << shift)$
  \EndIf
  \State $\mathsf{byteArray[index]} \gets byte$
\EndFunction
\vskip 10pt
\end{algorithmic}
\end{algorithm}

\section{Search Module}
\label{sec:searchmodule}

So we have our documents and we can access them without revealing which ones we are accessing. Now the final piece of the puzzle is actually performing search. We discuss building an Inverted Index, the data structure that will allow us to search efficiently, in \cref{subsec:invertedindex}. We then discuss the front-end of the whole application, the search API, in \cref{subsec:searchapi}.

\subsection{Inverted Index}
\label{subsec:invertedindex}

The basics of Inverted Indexes are discussed in \cref{sec:invertedindexintro}. As stated there, the Index consists of two main structures, the dictionary and the postings. For the dictionary, we will do the usual thing, which is to implement it as a hash table. This provides us with $O(1)$ lookup and insertion, which are the main operations we will be performing. The postings are more flexible. In our implementation we want to store the file names, because they are not actually stored in the filesystem itself. We also want to perform intersection of postings lists, allowing us to perform phrase search. Thus, we will use a Hash Set, a data structure built on top of a hash table, that stores a set of keys, and has the added benefit of keeping them unique for us.

Having decided on our data structure, we need to actually index files. As usual in this project, the files will be \texttt{Cstruct}s. So we have a number of steps to perform in order to process a file. We first convert the file to a string and immediately strip it of characters we don't need, pretty much all punctuation. At this point we have a sequence of alphanumeric character strings, separated by spaces and newlines, so we can perform a split on these characters to get a list of words.

We could perform indexing now, adding the name of the current file to the postings list of every word in our list, but there are a couple of things that we want to do first. We really don't want to store separate words for `run', `ran', `runs', and so on, so we will perform some linguistic preprocessing, stemming the words using Porter's stemming algorithm. I used a small open-source library implementation of this algorithm. This technique not only reduces the size of the index, but also arguably improves search, because now queries for 'run' can automatically returns documents containing any morphological derivation.

Finally, we remove duplicates from our list of words, reducing insertion overhead, and put the entires into the index.

For our purposes we will only implement simple conjunctive phrase search, meaning we look for documents containing all of the words in a space separated query string. Now search is as simple as performing the same preprocessing on the query string, performing lookup on each of the queries in turn and taking the big intersection of the resulting list of hash sets. In order to make this intersection operation efficient, we sort the list by order of hash set size. Then we can filter the smaller hash set by checking for membership in the larger hash set. This means we perform one constant time lookup for each member of the small set, rather than the large, giving a large performance boost, as this smaller set is monotonically decreasing.

\subsection{Keyword Search API}
\label{subsec:searchapi}

The final step in an end-to-end system is the API. We need to wrap file system access, indexing and search all into one module that provides a single point of entry for an encrypted search system.

We have three main operations that are the most important to support. Writing files, reading files, and searching over files. We will not be concerned with deleting files at the moment, but it is left as an extension if time permits.

Writing files is the most important step, because this is where the search module does the indexing. On write, we first write through to the first system, then index the file and finally flush the index to disk to make sure it persists. Reading files is simply a pass through to the file system and search makes calls to inverted index.

\section{Encryption}
\label{sec:encryption}

Encryption is something that it can be fatal to get wrong. In the case of ORAM, if you don't have encryption, it's completely superfluous. Thus, we want to use tested and trusted cryptographic libraries, rather than rolling our own. Luckily for us, there is an existing implementation of an encrypted block device satisfying Mirage's \texttt{BLOCK} interface, that provides a functor very similar to the ones that we created for ORAM. We simply have to chain together the application of this functor with the ones that we are already using to get an encrypted block device.

\chapter{Evaluation}

% ### Functionality ###

% Show that the unit tests show that individual components do what they are specified to do

% Path ORAM reads in and out correctly

% Encryption does make things encrypted

% Search is sound and complete...

% Etc.

% Use QuickCheck to build randomised testing

% ### Performance ###

% Evaluate the performance of the code and do it with different configurations, varying functors used, size of input, potentially simulated network latency etc.

% ### Security ###

% Use statistical methods to show that access pattern is easily visible to Bob before applying ORAM to the system and the show statistical evidence the access pattern is hidden after applying ORAM

% Show specifically that with the entire access pattern, the server can infer which documents are returned by each query, and could therefore perform the attack described by Islam et al.

\section{Overall Results}

\mytodo{Give an overview of the results and how things compare to expectations}

\section{Functional Testing}

\subsection{Unit Tests}

\mytodo{Talk about unit tests and how they were useful throughout the development process}

\subsection{Randomised Testing}

\mytodo{Talk about the use of Quickcheck to push functional testing further}

\section{Performance Testing}

\subsection{Microbenchmarks}

\mytodo{Talk about microbenchmarks, identifying part of the system that are taking time/memory}

\subsection{Comparison with Literature}
\label{subsec:comparisonWithLiterature}

\mytodo{Compare the results of microbenchmarking with theoretical bounds on speed, bandwidth and stash size given in the literature}

\subsection{File System Benchmarking}
\label{subsec:fsBenchmarking}

\mytodo{Talk about testing the file system (and therefore ORAM) on standardised workloads, allowing us to compare with other file systems in a useful way}

\section{Security Analysis}

\mytodo{Discuss how to perform security analysis using statistical tests}
\mytodo{Show results of security analysis}

\chapter{Conclusion}

\mytodo{Sum up main results of the dissertation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the bibliography
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{refs}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% the appendices
\appendix

\chapter{Project Proposal}

\input{proposal}

\end{document}
